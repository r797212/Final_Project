{"cells":[{"cell_type":"markdown","metadata":{"id":"afVI58wl5pn1"},"source":["# 0. Set Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyQxv1QYclw8"},"outputs":[],"source":["batch_size = 5\n","num_workers = 2\n","seed = 814"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SUPuFuY5u47"},"outputs":[],"source":["coco_dataset_folder = \"/content/drive/MyDrive/Final_Project/COCO_DATASET\"\n","MODEL_SAVE_PATH = ''"]},{"cell_type":"markdown","metadata":{"id":"UYjvEgwe5Ku6"},"source":["# 1. Dataset loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLiSjy22-Fvu"},"outputs":[],"source":["!pip install fastai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcsdKPJx-mWc"},"outputs":[],"source":["# Download and build dataset\n","'''\n","import os\n","from google.colab import drive\n","from fastai.data.external import untar_data, URLs\n","\n","drive.mount('/content/drive')\n","download_path = untar_data(URLs.COCO_SAMPLE, data=dataset_path)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJwu7793JMg6"},"outputs":[],"source":["'''\n","st = os.path.join(download_path, 'train_sample')\n","TRAIN_DATASET_PATH = '/content/drive/MyDrive/Final_Project/COCO_DATASET/train'\n","VAL_DATASET_PATH = '/content/drive/MyDrive/Final_Project/COCO_DATASET/val'\n","TEST_DATASET_PATH = '/content/drive/MyDrive/Final_Project/COCO_DATASET/test'\n","os.makedirs(TRAIN_DATASET_PATH)\n","os.makedirs(VAL_DATASET_PATH)\n","os.makedirs(TEST_DATASET_PATH)\n","\n","all_jpg_file = []\n","for dir in os.listdir(st):\n","    if os.path.isfile(os.path.join(st, dir)) and ('jpg' in dir):\n","        all_jpg_file.append(dir)\n","\n","from sklearn.model_selection import train_test_split\n","import shutil\n","\n","remain, test = train_test_split(all_jpg_file, test_size=0.1)\n","train, val = train_test_split(remain, test_size = 0.3)\n","print(len(all_jpg_file))\n","print(len(train))\n","print(len(val))\n","print(len(test))\n","def move_files(ARR, DIR, target_DIR):\n","    for jpg in ARR:\n","        c_DIR = os.path.join(DIR, jpg)\n","        shutil.move(c_DIR, os.path.join(target_DIR, jpg))\n","move_files(train, st, TRAIN_DATASET_PATH)\n","move_files(val, st, VAL_DATASET_PATH)\n","move_files(test, st, TEST_DATASET_PATH)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13596,"status":"ok","timestamp":1702958167327,"user":{"displayName":"Ruoyu Wang","userId":"14471031145395427491"},"user_tz":300},"id":"xsoLA6Td-8eQ","outputId":"43e6f222-0ff5-4e2f-b767-31470a991bb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os, shutil, sys\n","\n","drive.mount('/content/drive')\n","#os.makedirs(MODEL_SAVE_PATH, exist_ok= False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQnUgJCa-yke"},"outputs":[],"source":["# sys.path.append('/content/drive/MyDrive/Final_Project')\n","\n","# COCO_DATA_DIR = '/content/coco_dataset'\n","\n","# print('Copying training dataset to machine disk')\n","# shutil.copytree(coco_dataset_folder, COCO_DATA_DIR)\n","# TRAIN_DATASET_PATH = os.path.join(COCO_DATA_DIR, 'train')\n","# VAL_DATASET_PATH = os.path.join(COCO_DATA_DIR, 'val')\n","# TEST_DATASET_PATH = os.path.join(COCO_DATA_DIR, 'test')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rTFo9k6-yXO"},"outputs":[],"source":["COCO_DATA_DIR = coco_dataset_folder\n","\n","TRAIN_DATASET_PATH = os.path.join(COCO_DATA_DIR, 'train')\n","VAL_DATASET_PATH = os.path.join(COCO_DATA_DIR, 'val')\n","TEST_DATASET_PATH = os.path.join(COCO_DATA_DIR, 'test')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvPERY5metlr"},"outputs":[],"source":["import os, torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","\n","from skimage.color import rgb2lab, lab2rgb\n","import numpy as np\n","\n","\n","class COCO_Dataset(Dataset):\n","    def __init__(self, ALL_IMG_PATH, BASE_PATH, purpose, transform = None):\n","        assert(purpose in ['train', 'val', 'test'])\n","        self.H = 256\n","\n","        if purpose in ['val', 'test']:\n","            if transform == None:\n","                self.transform = transforms.Resize((self.H, self.H), transforms.InterpolationMode.BICUBIC)\n","            else:\n","                self.transform = transform\n","\n","        else:\n","            # Train\n","            if transform == None:\n","                self.transform = transforms.Resize((self.H, self.H), transforms.InterpolationMode.BICUBIC)\n","            else:\n","                self.transform = transform\n","\n","        self.purpose = purpose\n","        self.ALL_IMG_PATH = ALL_IMG_PATH\n","        self.BASE_PATH = BASE_PATH\n","\n","    def __len__(self):\n","        return len(self.ALL_IMG_PATH)\n","\n","    def _normalize_L_channel(self, l_channel):\n","        assert(l_channel.shape[0] == 1)\n","        return (l_channel / 50.0) - 1.0\n","\n","    def _normalize_AB_channel(self, AB_channel):\n","        assert(AB_channel.shape[0] == 2)\n","        return (AB_channel / 110.0)\n","\n","    def __getitem__(self, index):\n","        c_image_path = os.path.join(self.BASE_PATH, self.ALL_IMG_PATH[index])\n","        img = Image.open(c_image_path)\n","        img = img.convert('RGB')\n","        img = self.transform(img)\n","        img = np.array(img)\n","\n","        # COnvert RGB to LAB for easier\n","        lab_img = torch.from_numpy(rgb2lab(img)).float().permute(2,0,1)\n","        print(lab_img.shape)\n","        L_ch = lab_img[[0], ...]\n","        AB_ch = lab_img[[1,2], ...]\n","        L_ch = self._normalize_L_channel(L_ch)\n","        AB_ch = self._normalize_AB_channel(AB_ch)\n","        print(L_ch.shape)\n","        return (L_ch, AB_ch)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCbdUfEFcufP"},"outputs":[],"source":["train_imgs = os.listdir(TRAIN_DATASET_PATH)\n","train_Dataset = COCO_Dataset(ALL_IMG_PATH = train_imgs, BASE_PATH=TRAIN_DATASET_PATH,\n","                             purpose='train')\n","train_dataloader = DataLoader(train_Dataset, batch_size = batch_size, shuffle=True,\n","                              num_workers = num_workers, pin_memory=True, drop_last=True)\n","\n","val_imgs = sorted(os.listdir(VAL_DATASET_PATH))\n","val_Dataset = COCO_Dataset(ALL_IMG_PATH = val_imgs, BASE_PATH=VAL_DATASET_PATH,\n","                             purpose='val')\n","val_dataloader = DataLoader(val_Dataset, batch_size = batch_size, shuffle=False,\n","                              num_workers = num_workers, pin_memory=True, drop_last=True)\n"]},{"cell_type":"markdown","metadata":{"id":"XeDSTAaGlnMa"},"source":["# 2. Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1702958218291,"user":{"displayName":"Ruoyu Wang","userId":"14471031145395427491"},"user_tz":300},"id":"O94IPsAW_K_q","outputId":"79bd4b5f-9dbf-4b50-8add-22a3446217fd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7d8224d035b0>"]},"metadata":{},"execution_count":11}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","\n","torch.manual_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1kCX8pGloVa"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","\n","\n","# --- UNet --- #\n","# credit to https://github.com/milesial/Pytorch-UNet for the unet implementation\n","\n","class UNet(nn.Module):\n","    def __init__(self, in_channels, out_im_channels, batchnorm = True, dropout=0.3, bc=64):\n","        super(UNet, self).__init__()\n","\n","        self.inc = inconv(in_channels, bc*1, batchnorm)\n","        self.down1 = down(bc*1, bc*2, batchnorm, dropout=dropout)\n","        self.down2 = down(bc*2, bc*4, batchnorm, dropout=dropout)\n","        self.down3 = down(bc*4, bc*8, batchnorm, dropout=dropout)\n","        self.down4 = down(bc*8, bc*8, batchnorm, dropout=dropout)\n","        self.up1 = up(bc*16, bc*4, batchnorm, dropout=dropout)\n","        self.up2 = up(bc*8, bc*2, batchnorm, dropout=dropout)\n","        self.up3 = up(bc*4, bc*1, batchnorm, dropout=dropout)\n","        self.up4 = up(bc*2, bc*2, batchnorm, dropout=dropout)\n","        self.outc = outconv(bc*2, out_im_channels,)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x = x5 = self.down4(x4)\n","\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","\n","        return self.outc(x)\n","\n","# --- helper modules --- #\n","def convrelu(in_channels, out_channels, kernel, padding):\n","    return nn.Sequential(\n","      nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n","      nn.ReLU(inplace=True),\n","    )\n","\n","\n","class double_conv(nn.Module):\n","    '''(conv => BN => ReLU) * 2'''\n","    def __init__(self, in_ch, out_ch, batchnorm=True):\n","        super(double_conv, self).__init__()\n","        if batchnorm:\n","            self.conv = nn.Sequential(\n","              nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n","              nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True))\n","        else:\n","            self.conv = nn.Sequential(\n","              nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n","              nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True))\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class inconv(nn.Module):\n","    def __init__(self, in_ch, out_ch, batchnorm):\n","        super(inconv, self).__init__()\n","        self.conv = double_conv(in_ch, out_ch, batchnorm)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class down(nn.Module):\n","    def __init__(self, in_ch, out_ch, batchnorm, dropout=None):\n","        super(down, self).__init__()\n","        self.mpconv = nn.Sequential(nn.MaxPool2d(2), double_conv(in_ch, out_ch, batchnorm))\n","\n","        if dropout:\n","            self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.mpconv(x)\n","\n","        if self.dropout:\n","            x = self.dropout(x)\n","        return x\n","\n","\n","class up(nn.Module):\n","    def __init__(self, in_ch, out_ch, batchnorm, method='conv', dropout=None):\n","        super(up, self).__init__()\n","\n","        if method == 'bilinear':\n","            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        elif method == 'conv':\n","            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n","        elif method == 'upconv':\n","            self.up = nn.Sequential(\n","                nn.Upsample(scale_factor=2, mode='bilinear'),\n","                nn.ReflectionPad2d(1),\n","                # note the interesting size and stride\n","                nn.Conv2d(in_ch // 2, in_ch // 2, kernel_size=2, stride=2, padding=0),\n","            )\n","        elif method == 'none':\n","            self.up = nn.Identity()\n","\n","        self.conv = double_conv(in_ch, out_ch, batchnorm)\n","\n","        if dropout:\n","            self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        # up conv here\n","\n","        # input is CHW\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2))\n","\n","        # for padding issues, see\n","        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n","        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n","\n","        x = torch.cat([x2, x1], dim=1)\n","        x = self.conv(x)\n","\n","        if self.dropout:\n","            x = self.dropout(x)\n","\n","        return x\n","\n","\n","class outconv(nn.Module):\n","    def __init__(self, in_ch, out_ch,):\n","        super(outconv, self).__init__()\n","        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n","\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43s8Orz9lXaZ"},"outputs":[],"source":["from fastai.vision.learner import create_body\n","from torchvision.models.resnet import resnet18\n","from fastai.vision.models.unet import DynamicUnet\n","\n","def generate_pretrain_unet(n_input=1, n_output=2, size=256):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    body = create_body(resnet18(), pretrained=True, n_in=n_input, cut=-2)\n","    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n","    return net_G\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XtzzOwH_mxBR"},"outputs":[],"source":["# Discriminator\n","\n","class dcgan_Discriminator(nn.Module):\n","    def __init__(self):\n","\n","        self.m = None\n","\n","class Patch_Discriminator(nn.Module):\n","    def __init__(self, in_channels, bc=64):\n","        super().__init__()\n","        layer = []\n","\n","        # step 0\n","        s_0 = [nn.Conv2d(in_channels, bc, 4, 2, 1, bias=False)]\n","        s_0.append(nn.LeakyReLU(0.2, inplace=True))\n","        s_0 = nn.Sequential(*s_0)\n","        layer.append(s_0)\n","\n","        # step 1\n","        s_1 = [nn.Conv2d(bc, bc*2, 4, 2, 1, bias=False)]\n","        s_1.append(nn.BatchNorm2d(bc*2))\n","        s_1.append(nn.LeakyReLU(0.2, inplace=True))\n","        s_1 = nn.Sequential(*s_1)\n","        layer.append(s_1)\n","\n","        # step 2\n","        s_2 = [nn.Conv2d(bc*2, bc*4, 4, 2, 1, bias=False)]\n","        s_2.append(nn.BatchNorm2d(bc*4))\n","        s_2.append(nn.LeakyReLU(0.2, inplace=True))\n","        s_2 = nn.Sequential(*s_2)\n","        layer.append(s_2)\n","\n","        # step 3\n","        s_3 = [nn.Conv2d(bc*4, bc*8, 4, 1, 1, bias=False)]\n","        s_3.append(nn.BatchNorm2d(bc*8))\n","        s_3.append(nn.LeakyReLU(0.2, inplace=True))\n","        s_3 = nn.Sequential(*s_3)\n","        layer.append(s_3)\n","\n","        # step 4\n","        s_4 = [nn.Conv2d(bc*8, 1, 4,1,1,)]\n","        s_4 = nn.Sequential(*s_4)\n","        layer.append(s_4)\n","\n","        self.layers = nn.Sequential(*layer)\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMnT0YTE3-ce"},"outputs":[],"source":["# Loss\n","class loss_GAN(nn.Module):\n","    def __init__(self,):\n","        super().__init__()\n","        self.loss = nn.BCEWithLogitsLoss()\n","\n","    def calculate_loss(self, pred, isreal):\n","        gt_label = None\n","        if isreal == True:\n","            gt_label = torch.ones_like(pred, dtype=pred.dtype, device=pred.device)\n","        else:\n","            gt_label = torch.zeros_like(pred, dtype=pred.dtype, device=pred.device)\n","        return self.loss(pred, gt_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VVEqbfFgOCGI"},"outputs":[],"source":["def denorm_l_channel(l_channel):\n","    return (l_channel + 1.0) * 50.0\n","\n","def denorm_ab_channel(ab_channel):\n","    return ab_channel * 110.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODM0TrLjS8c3"},"outputs":[],"source":["class Patch_GAN(nn.Module):\n","    def __init__(self, G_model, D_model, lr, loss_lambda = 100.):\n","        super().__init__()\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","        # self.G_model = G_model\n","        self.G_model = self.initialize_and_to_device(G_model, self.device)\n","        self.D_model = self.initialize_and_to_device(D_model, self.device)\n","\n","        #self.G_model = self.G_model.to(self.device)\n","        #self.D_model = self.D_model.to(self.device)\n","        self.optim_G = optim.AdamW(self.G_model.parameters(), lr = lr, betas=(0.5, 0.999), weight_decay=1e-5)\n","        self.optim_D = optim.AdamW(self.D_model.parameters(), lr = lr, betas=(0.5, 0.999), weight_decay=1e-5)\n","        self.optim_G_only = optim.AdamW(self.G_model.parameters(), lr = lr, betas=(0.5, 0.999), weight_decay=1e-5)\n","        self.l1_loss = nn.L1Loss()\n","        self.GAN_loss = loss_GAN()\n","        self.loss_lambda = loss_lambda\n","\n","    # Predict fake ab channel\n","    def forward(self, l_channel):\n","        fake_ab = self.G_model(l_channel)\n","        return fake_ab\n","\n","    def whether_train_model(self, model, whether_train):\n","        for param in model.parameters():\n","            param.requires_grad = whether_train\n","\n","    def train_step(self, l_channel, ab_channel):\n","        l_channel = l_channel.to(self.device)\n","        ab_channel = ab_channel.to(self.device)\n","\n","        fake_ab = self.forward(l_channel)\n","        # Train Discriminator\n","        self.D_model.train()\n","        self.whether_train_model(self.D_model, whether_train=True)\n","        self.optim_D.zero_grad()\n","\n","        # Backward on Discriminator\n","        gt_img = torch.cat((l_channel, ab_channel), dim=1)\n","        fake_img = torch.cat((l_channel, fake_ab), dim=1)\n","        D_pred_fake = self.D_model(fake_img.detach())\n","        D_loss_fake = self.GAN_loss.calculate_loss(pred=D_pred_fake, isreal=False)\n","        D_pred_gt = self.D_model(gt_img)\n","        D_loss_real = self.GAN_loss.calculate_loss(pred=D_pred_gt, isreal=True)\n","\n","        D_loss_total = 0.5*D_loss_fake + 0.5*D_loss_real\n","        D_loss_total.backward()\n","        self.optim_D.step()\n","\n","        # Backward on Generator, Freeze Generator\n","        self.G_model.train()\n","        self.whether_train_model(self.D_model, whether_train=False)\n","        self.optim_G.zero_grad()\n","\n","        fake_img = torch.cat((l_channel, fake_ab), dim=1)\n","        G_pred_fake = self.D_model(fake_img)\n","\n","        G_loss_GAN = self.GAN_loss.calculate_loss(pred=G_pred_fake, isreal = True)\n","        G_loss_L1 = self.l1_loss(fake_ab, ab_channel)\n","        G_loss_total = G_loss_GAN + G_loss_L1*self.loss_lambda\n","        G_loss_total.backward()\n","        self.optim_G.step()\n","\n","        return [D_loss_fake.cpu().item(), D_loss_real.cpu().item(), D_loss_total.cpu().item(),\n","                G_loss_GAN.cpu().item(), G_loss_L1.cpu().item(), G_loss_total.cpu().item()]\n","\n","    def visualize(self, l_channel, ab_channel, epoch=0):\n","        l_channel = l_channel.to(self.device)\n","        ab_channel = ab_channel.to(self.device)\n","        self.G_model.eval()\n","        with torch.no_grad():\n","            fake_ab = self.forward(l_channel).detach()\n","            fake_img = torch.cat((denorm_l_channel(l_channel), denorm_ab_channel(fake_ab)), dim=1).detach()\n","            gt_img = torch.cat((denorm_l_channel(l_channel), denorm_ab_channel(ab_channel)), dim=1)\n","\n","            fake_img = fake_img.permute(0,2,3,1).cpu().numpy()\n","            gt_img = gt_img.permute(0,2,3,1).cpu().numpy()\n","\n","            all_fake_rgb = []\n","            all_gt_rgb = []\n","\n","            for i in range(fake_img.shape[0]):\n","                c_fake_img = fake_img[i]\n","                c_gt_img = gt_img[i]\n","\n","                c_fake_rgb = lab2rgb(c_fake_img)\n","                c_gt_rgb = lab2rgb(c_gt_img)\n","\n","                all_fake_rgb.append(c_fake_rgb)\n","                all_gt_rgb.append(c_gt_rgb)\n","\n","        self.G_model.train()\n","\n","        # Make plots:\n","        img_num = 5\n","        vis_fig = plt.figure(figsize=(9,16))\n","        for i in range(img_num):\n","            c_grey_img = l_channel[i][0].detach().cpu().numpy()\n","            vis = plt.subplot(img_num, 3, 1+ i*3)\n","            vis.imshow(c_grey_img, cmap='gray')\n","            vis.axis('off')\n","\n","            vis = plt.subplot(img_num, 3, 2+ i*3)\n","            vis.imshow(all_fake_rgb[i])\n","            vis.axis('off')\n","\n","            vis = plt.subplot(img_num, 3, 3+ i*3)\n","            vis.imshow(all_gt_rgb[i])\n","            vis.axis('off')\n","        plt.show()\n","        # plt.savefig(os.path.join(MODEL_SAVE_PATH, f\"epoch={epoch}.png\"))\n","\n","    def save_model(self, save_path, epoch):\n","        save_path = os.path.join(save_path, f'epoch={epoch}.pt')\n","        torch.save({\n","            'epoch': epoch,\n","            'G_state_dict': self.G_model.state_dict(),\n","            'D_state_dict': self.D_model.state_dict(),\n","            'opt_G_state_dict': self.optim_G.state_dict(),\n","            'opt_D_state_dict': self.optim_D.state_dict(),\n","        }, save_path)\n","\n","    def load_model(self, load_DIR):\n","        CHECKPOINT = torch.load(load_DIR)\n","        self.G_model.load_state_dict(CHECKPOINT['G_state_dict'])\n","        self.D_model.load_state_dict(CHECKPOINT['D_state_dict'])\n","        self.optim_G.load_state_dict(CHECKPOINT['opt_G_state_dict'])\n","        self.optim_D.load_state_dict(CHECKPOINT['opt_D_state_dict'])\n","\n","    def weight_initialization(self, model, ):\n","        for m in model.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","        return model\n","\n","    def initialize_and_to_device(self, model, device):\n","        model = model.to(device)\n","        model = self.weight_initialization(model)\n","        return model\n","\n","    def train_generator_only(self, l_channel, ab_channel, ):\n","        self.G_model.train()\n","        l_channel = l_channel.to(self.device)\n","        ab_channel = ab_channel.to(self.device)\n","        self.optim_G_only.zero_grad()\n","\n","        fake_ab = self.forward(l_channel)\n","        G_loss = self.l1_loss(fake_ab, ab_channel)\n","        G_loss.backward()\n","        self.optim_G_only.step()\n","\n","        return G_loss.cpu().item()"]},{"cell_type":"markdown","metadata":{"id":"ov8DjJAIVBqZ"},"source":["# Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UqArAbkoVDKf"},"outputs":[],"source":["def train(model, epoch, ):\n","    visualization_batch = next(iter(val_dataloader))\n","    vis_l, vis_ab = visualization_batch\n","\n","    best_G_loss = 99999999\n","\n","    loss_log = []\n","    loss_log_save_DIR = os.path.join(MODEL_SAVE_PATH, 'loss.npy')\n","\n","    for epoch_num in range(epoch):\n","        loss_arr = []\n","        for c_data in tqdm(train_dataloader):\n","            c_l_channel, c_ab_channel = c_data\n","            c_loss = model.train_generator_only(c_l_channel, c_ab_channel)\n","            loss_arr.append(c_loss)\n","        loss_arr = np.array(loss_arr)\n","        loss_arr = np.mean(loss_arr, axis=0)\n","\n","        loss_log.append(loss_arr)\n","        np.save(loss_log_save_DIR, np.array(loss_log))\n","\n","        print(loss_arr)\n","\n","        if epoch_num % 5 == 0:\n","            print(f'Epoch: {epoch_num}')\n","            model.visualize(vis_l, vis_ab, epoch_num)\n","            model.save_model(MODEL_SAVE_PATH, epoch_num)\n","\n","    # for epoch_num in range(epoch):\n","    #     loss_arr = []\n","\n","    #     for c_data in tqdm(train_dataloader):\n","    #         c_l_channel, c_ab_channel = c_data\n","    #         c_loss_arr = model.train_step(c_l_channel, c_ab_channel)\n","\n","    #         loss_arr.append(c_loss_arr)\n","    #     loss_arr = np.array(loss_arr)\n","    #     loss_arr = np.mean(loss_arr, axis=0)\n","\n","    #     loss_log.append(loss_arr)\n","    #     np.save(loss_log_save_DIR, np.array(loss_log))\n","\n","    #     epoch_G_loss = loss_arr[-1]\n","    #     if epoch_G_loss < best_G_loss:\n","    #         # model.save_model(MODEL_SAVE_PATH, epoch_num)\n","    #         best_G_loss = epoch_G_loss\n","\n","\n","    #     if epoch_num % 5 == 0:\n","    #         print(f'Epoch: {epoch_num}')\n","    #         model.visualize(vis_l, vis_ab, epoch_num)\n","    #         model.save_model(MODEL_SAVE_PATH, epoch_num)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXmE2cvHSXKH"},"outputs":[],"source":["# G_model = Unet(1, 2, )\n","G_model = generate_pretrain_unet(n_input=1, n_output=2, size=256)\n","D_model = Patch_Discriminator(3)\n","model = Patch_GAN(G_model, D_model, 1e-4)\n","train(model, 100)"]},{"cell_type":"markdown","source":["# Visualization"],"metadata":{"id":"dvELRYovN34u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-cUxl6zQRNxN"},"outputs":[],"source":["model_path = ''\n","G_model = generate_pretrain_unet(n_input=1, n_output=2, size=256)\n","D_model = Patch_Discriminator(3)\n","model = Patch_GAN(G_model, D_model, 1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yo7ZZM2nHl4E"},"outputs":[],"source":["model_path = ''\n","\n","G_model = Unet(1, 2, )\n","D_model = Patch_Discriminator(3)\n","model = Patch_GAN(G_model, D_model, 1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVfpPGVnbZmu"},"outputs":[],"source":["from torchvision import transforms\n","\n","batch_size = 5\n","num_workers = 8\n","\n","\n","train_imgs = ['000000179025.jpg', '000000331331.jpg', '000000156323.jpg', '000000562305.jpg', '000000222908.jpg']\n","train_Dataset = COCO_Dataset(ALL_IMG_PATH = train_imgs, BASE_PATH=TRAIN_DATASET_PATH,\n","                             purpose='train', )\n","train_dataloader = DataLoader(train_Dataset, batch_size = batch_size, shuffle=False,\n","                              num_workers = num_workers, pin_memory=True, drop_last=True)\n","\n","val_imgs = ['000000002347.jpg', '000000002429.jpg', '000000002444.jpg', '000000004125.jpg', '000000005115.jpg']\n","val_Dataset = COCO_Dataset(ALL_IMG_PATH = val_imgs, BASE_PATH=VAL_DATASET_PATH,\n","                             purpose='val', )\n","val_dataloader = DataLoader(val_Dataset, batch_size = batch_size, shuffle=False,\n","                              num_workers = num_workers, pin_memory=True, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUr7BNyibcmO"},"outputs":[],"source":["print(\"training set\")\n","for c_data in tqdm(train_dataloader):\n","    vis_l, vis_ab = c_data\n","    model.load_model(model_path)\n","    model.visualize(vis_l, vis_ab)\n","\n","print(\"validation set\")\n","for c_data in tqdm(val_dataloader):\n","    vis_l, vis_ab = c_data\n","    model.visualize(vis_l, vis_ab)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}